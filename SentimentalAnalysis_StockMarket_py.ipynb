{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPb+VCeCjeOKA7YKF3/pY5o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShauryaRawat10/Artificial-Intelligence/blob/main/SentimentalAnalysis_StockMarket_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement\n",
        "Business Context\n",
        "The prices of the stocks of companies listed under a global exchange are influenced by a variety of factors, with the company's financial performance, innovations and collaborations, and market sentiment being factors that play a significant role. News and media reports can rapidly affect investor perceptions and, consequently, stock prices in the highly competitive financial industry. With the sheer volume of news and opinions from a wide variety of sources, investors and financial analysts often struggle to stay updated and accurately interpret its impact on the market. As a result, investment firms need sophisticated tools to analyze market sentiment and integrate this information into their investment strategies.\n",
        "\n",
        "## Problem Definition\n",
        "With an ever-rising number of news articles and opinions, an investment startup aims to leverage artificial intelligence to address the challenge of interpreting stock-related news and its impact on stock prices. They have collected historical daily news for a specific company listed under NASDAQ, along with data on its daily stock price and trade volumes.\n",
        "\n",
        "As a member of the Data Science and AI team in the startup, you have been tasked with analyzing the data, developing an AI-driven sentiment analysis system that will automatically process and analyze news articles to gauge market sentiment, and summarizing the news at a weekly level to enhance the accuracy of their stock price predictions and optimize investment strategies. This will empower their financial analysts with actionable insights, leading to more informed investment decisions and improved client outcomes.\n",
        "\n",
        "## Data Dictionary\n",
        "- Date : The date the news was released\n",
        "- News : The content of news articles that could potentially affect the company's stock price\n",
        "- Open : The stock price `(in $)` at the beginning of the day\n",
        "- High : The highest stock price `(in $)` reached during the day\n",
        "- Low : The lowest stock price `(in $)` reached during the day\n",
        "- Close : The adjusted stock price `(in $)` at the end of the day\n",
        "- Volume : The number of shares traded during the day\n",
        "- Label : The sentiment polarity of the news content\n",
        "  - 1: positive\n",
        "  - 0: neutral\n",
        "  - -1: negative"
      ],
      "metadata": {
        "id": "kRt_d3TwQSLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing and Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "hrAeF7TEQ3wy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Aqrl9-QKbh4",
        "outputId": "32f8e914-8019-453d-9195-460c70de4cc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers==2.7.0\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting transformers==4.40.2\n",
            "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.46.0\n",
            "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Collecting accelerate==1.7.0\n",
            "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting sentencepiece==0.2.0\n",
            "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting pandas==2.2.2\n",
            "  Downloading pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting numpy==2.0.2\n",
            "  Downloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib==3.10.0\n",
            "  Downloading matplotlib-3.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting seaborn==0.13.2\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting torch==2.6.0\n",
            "  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting scikit-learn==1.6.1\n",
            "  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --force-reinstall sentence-transformers==2.7.0 transformers==4.40.2 bitsandbytes==0.46.0 accelerate==1.7.0 sentencepiece==0.2.0 pandas==2.2.2 numpy==2.0.2 matplotlib==3.10.0 seaborn==0.13.2 torch==2.6.0 scikit-learn==1.6.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e6ec25c"
      },
      "source": [
        "%pip install gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To manipulate and analyze data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# To visualize data\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# To used time-related functions\n",
        "import time\n",
        "\n",
        "# To parse JSON data\n",
        "import json\n",
        "\n",
        "# To build, tune, and evaluate ML models\n",
        "# from sklearn.ensemble import DecisionTreeClassifier # Incorrecct import\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier # Update import replacing commented sklearn.ensemble import\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# To load/create word embeddings\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "# To work with transformer models\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# To implement progress bar related functionalities\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "# To ignore unnecessary warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "u_8mjdFENFx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check library versions\n",
        "print(\"pandas:   \", pd.__version__)\n",
        "print(\"numpy:    \", np.__version__)\n",
        "print(\"seaborn:  \", sns.__version__)\n",
        "print(\"torch:    \", torch.__version__)"
      ],
      "metadata": {
        "id": "xqGBMeuBfp9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset"
      ],
      "metadata": {
        "id": "WT63ugG9RAht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_news = pd.read_csv('https://raw.githubusercontent.com/ShauryaRawat10/Data-Science/main/Generative%20AI/Storage/stock_news.csv', engine='python')"
      ],
      "metadata": {
        "id": "hlppxBNKNqwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock = stock_news.copy()"
      ],
      "metadata": {
        "id": "4aaklcS_gWPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Overview"
      ],
      "metadata": {
        "id": "tRzfzGdvRH-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock.head()"
      ],
      "metadata": {
        "id": "yV7QOL6hO9_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock.tail()"
      ],
      "metadata": {
        "id": "qKh-dDq6PQzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock.shape"
      ],
      "metadata": {
        "id": "8iatltX9PS23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Total 349 rows and 8 columns in the Stock market dataset"
      ],
      "metadata": {
        "id": "dAm0viT_TWYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock.columns"
      ],
      "metadata": {
        "id": "yM3ZnqSDPg7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock.duplicated().sum()"
      ],
      "metadata": {
        "id": "emxeFyXiUVtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- No duplicates"
      ],
      "metadata": {
        "id": "QvDwPQGOUaN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock.isnull().sum()"
      ],
      "metadata": {
        "id": "s52fTSgJUY4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- No null values"
      ],
      "metadata": {
        "id": "aSS2HNx2Uen1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock.nunique()"
      ],
      "metadata": {
        "id": "UahAAbPWP3VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- News columns is always unique\n",
        "- Label (Sentiment) has 3 categories"
      ],
      "metadata": {
        "id": "RpeEFfv4Thyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock.info()"
      ],
      "metadata": {
        "id": "TZ_4eKYiSlS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 6 Columns are of numerical data type. 2 are categorical"
      ],
      "metadata": {
        "id": "uVuPsH56T9KD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert Date to DateTime format"
      ],
      "metadata": {
        "id": "UIvBOSndg17I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock['Date'] = pd.to_datetime(stock['Date'])"
      ],
      "metadata": {
        "id": "pt_PbcQSgzlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock.dtypes"
      ],
      "metadata": {
        "id": "gJ5PKW7qg9_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "stock.describe()"
      ],
      "metadata": {
        "id": "YjUztaNESnJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Observations:\n",
        "  - For 349 days in dataset, the Stock market has:\n",
        "    - Stock Volume of 244439200\n",
        "    - Highest Stock price: 67\n",
        "    - Lowest Stock price: 36.25"
      ],
      "metadata": {
        "id": "2rm9BQMyhP_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.countplot(data=stock, x='Label', stat=\"percent\")\n",
        "\n",
        "# Add percentage values at the top of each bar\n",
        "total = len(stock)\n",
        "                                       # Total number of items in the dataset\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.annotate(f'{height:.2f}%',                            # Display percentage with 2 decimals\n",
        "                (p.get_x() + p.get_width() / 2., height),    # Position at the top of the bar\n",
        "                ha='center', va='center',                    # Alignment\n",
        "                fontsize=8, color='black', fontweight='bold', # Styling\n",
        "                xytext=(0, 5), textcoords='offset points')   # Adjust text position"
      ],
      "metadata": {
        "id": "V5L1UcRUUh3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the reported news:\n",
        "- 48.7% with Neural Sentiment\n",
        "- 28.4% with Negative Sentiment\n",
        "- 22.9% with positive sentiment"
      ],
      "metadata": {
        "id": "Z_ATjPAuV3Y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=stock, x='Open')"
      ],
      "metadata": {
        "id": "cwigPptQWOvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Most of the stocks with 'Opening Price' are between 35 - 63 unit dollars, with few outliers of 65+"
      ],
      "metadata": {
        "id": "ptvGyoVZWeHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=stock, x='Close')"
      ],
      "metadata": {
        "id": "R93qCEtdWdfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Closing price for stocks are between 37 to 55 unit dollar. Outliers of 63-65 exists"
      ],
      "metadata": {
        "id": "xI96ak62XlfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Density Plot of Price (Open,High,Low,Close)\n",
        "for i in [\"Open\",\"High\",\"Low\",\"Close\"]:\n",
        "    sns.kdeplot(stock[i], label=i, shade=True)\n",
        "plt.xlabel(\"Price\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.title(\"Density Plot of Stock Prices\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "whh_K9apYExa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Stock Price mostly landed in the 40-50 range"
      ],
      "metadata": {
        "id": "xdThyF5XktCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x='Date', y='High', data=stock, color='blue', label='High')\n",
        "sns.lineplot(x='Date', y='Low', data=stock, color='red', label='Low')\n",
        "plt.title('Stocks High and Low price')\n",
        "plt.xticks(\n",
        "    rotation=45,                     # rotate for readability\n",
        "    ha='right'                       # right-align the labels\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7yNbCCJFbTPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap( stock.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=\".2f\" )"
      ],
      "metadata": {
        "id": "pqpoG-qNb6wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Open, Close, High, Low are highly correlated\n",
        "- Volume is negatively correlated with High, Open, Close, Low and Label"
      ],
      "metadata": {
        "id": "9J2a_cXJctjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock['news_len'] = stock['News'].apply(lambda x: len(x.split(' '))) # Calculating the total number of words present in the news content column.\n",
        "stock['news_len'].describe()                                         # Print the statistical summary for the news content length after splitting into words."
      ],
      "metadata": {
        "id": "E6dCWUyCdOd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Average words in news are around 50\n",
        "- Maximum words are 61 and minimum as 19"
      ],
      "metadata": {
        "id": "Hsqvr6Xslh0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "for i, variable in enumerate(['Open', 'High', 'Low', 'Close']):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    sns.boxplot(data=stock, x=\"Label\", y=variable) # Label = Sentiment (1) - Positive, (0) - Neutral, (-1) - Negative\n",
        "    plt.tight_layout(pad=2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gbZzRvB_mCpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(\n",
        "    data=stock, x=\"Label\", y=\"Volume\"  # # Label = Sentiment (1) - Positive, (0) - Neutral, (-1) - Negative\n",
        ");"
      ],
      "metadata": {
        "id": "IooXRtDjmVA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_daily = stock.groupby('Date').agg(\n",
        "    {\n",
        "        'Open': 'mean',\n",
        "        'High': 'mean',\n",
        "        'Low': 'mean',\n",
        "        'Close': 'mean',\n",
        "        'Volume': 'mean',\n",
        "    }\n",
        ").reset_index()                             # Group the 'stocks' DataFrame by the 'Date' column\n",
        "\n",
        "stock_daily.set_index('Date', inplace=True) # Index\n",
        "stock_daily.sample(n=10, random_state=42)   # Random selection of rows. But the same ones used whenever randon is chosen."
      ],
      "metadata": {
        "id": "c1Po-CoqnFWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,3))\n",
        "sns.lineplot(stock_daily.drop(\"Volume\", axis=1));"
      ],
      "metadata": {
        "id": "S4Yrp4fMm20L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There is a cyclic pattern (seasionality) with increase/descrease in prices of stock\n",
        "- In the first half of month the stock surges, in next half it drops and remains mostly low"
      ],
      "metadata": {
        "id": "xqrOfGdinQa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(15,5))      # Create a figure and axis\n",
        "sns.lineplot(data=stock_daily.reset_index(), x='Date', y='Close', ax=ax1, color='blue', marker='o', label='Close Price') # Lineplot on primary y-axis\n",
        "ax2 = ax1.twinx()                            # Create a secondary y-axis\n",
        "sns.lineplot(data=stock_daily.reset_index(), x='Date', y='Volume', ax=ax2, color='gray', marker='o', label='Volume') # Lineplot on secondary y-axis\n",
        "ax1.legend(bbox_to_anchor=(1,1));            # Legend set to the Volume data"
      ],
      "metadata": {
        "id": "0_cPV1KvnHrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qtz2ouYzUCVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock[\"Date\"].fillna(\"Unknown\", inplace=True) # Replace NaN with a Default Value (e.g., “Unknown” or a placeholder)\n",
        "stock[\"Date\"].describe()"
      ],
      "metadata": {
        "id": "642QgfUGsLAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Train Split"
      ],
      "metadata": {
        "id": "xzDeIXcVsexi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = stock[(stock['Date'] < '2019-04-01')].reset_index()    # Select all rows where the 'Date' is before '2019-04-01'\n",
        "X_val = stock[(stock['Date'] >= '2019-04-01') & (stock['Date'] < '2019-04-16')].reset_index()    # Select all rows where the 'Date' is from '2019-04-01 to '2019-04-16' (excluded)\n",
        "X_test = stock[stock['Date'] >= '2019-04-16'].reset_index()      # Select all rows where the 'Date' is from '2019-04-16' till the end."
      ],
      "metadata": {
        "id": "SwXlNJ7PsRp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'Label' column is the target variable (lower cse variables)\n",
        "y_train = X_train[\"Label\"].copy()\n",
        "y_val = X_val[\"Label\"].copy()\n",
        "y_test = X_test[\"Label\"].copy()"
      ],
      "metadata": {
        "id": "Ye8KUJ2usYSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shape of X_train,X_val,X_test,y_train,y_val and y_test\n",
        "print(\"\\nTrain data shape\",X_train.shape)\n",
        "print(\"Validation data shape\",X_val.shape)\n",
        "print(\"Test data shape \",X_test.shape)\n",
        "line= '_' * 25\n",
        "print(line)\n",
        "print(\"\\nTrain Label shape\",y_train.shape)\n",
        "print(\"Validation Label shape\",y_val.shape)\n",
        "print(\"Test Label shape \",y_test.shape)"
      ],
      "metadata": {
        "id": "Znm0zXrMshdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word Embeddings**"
      ],
      "metadata": {
        "id": "qZk5NR04sn6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1 - Word2Vec"
      ],
      "metadata": {
        "id": "YNESXgYbs1hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_list = [item.split(\" \") for item in stock['News'].values] # Creating a list of all words in our data"
      ],
      "metadata": {
        "id": "u1Q-AS1JskD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an instance of Word2Vec\n",
        "vec_size = 300 # Determines the number of features used to represent each word in the vector space. A higher vec_size can increase computational complexity as it captures more nuances.\n",
        "model_W2V = Word2Vec(words_list, vector_size = vec_size, min_count = 1, window=5, workers = 6) # Model will learn these embeddings by analyzing word co-occurrences within a context window of 5 words."
      ],
      "metadata": {
        "id": "C7wBH0bMs8Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "%md\n",
        "<h3>Word2Vec Parameters for our Model</h2>\n",
        "\n",
        "  <table>\n",
        "    <tr>\n",
        "      <th>Parameter</th>\n",
        "      <th>Description</th>\n",
        "      <th>Value</th>\n",
        "      <th>Comment</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>`vec_size`</td>\n",
        "      <td>Dimensionality of word vectors</td>\n",
        "      <td>300</td>\n",
        "      <td>It determines the number of features used to represent each word in the vector space.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>`model_W2V`</td>\n",
        "      <td>Word2Vec model instance</td>\n",
        "      <td>-</td>\n",
        "      <td>The Word2Vec model learns these representations by analyzing the co-occurrence patterns of words in the input text.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>`words_list`</td>\n",
        "      <td>Input data (list of sentences or words)</td>\n",
        "      <td>-</td>\n",
        "      <td>This argument represents the input data for the model. The model will learn word embeddings based on the words and their contexts within these sentences.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>`vector_size`</td>\n",
        "      <td>Dimensionality of word vectors</td>\n",
        "      <td>300</td>\n",
        "      <td>In this case, it is set to 300, meaning that **each word** will be represented by a vector with 300 dimensions.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>`min_count`</td>\n",
        "      <td>Minimum word frequency to be included</td>\n",
        "      <td>1</td>\n",
        "      <td>Specifies the minimum number of times a word must appear in the training data to be included in the model's vocabulary.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>`window`</td>\n",
        "      <td>Context window size</td>\n",
        "      <td>5</td>\n",
        "      <td>Context window around a target word. The model considers words within a window before and after the target word **to learn its vector representation**.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>`workers`</td>\n",
        "      <td>Number of worker threads</td>\n",
        "      <td>6</td>\n",
        "      <td>Using multiple workers can significantly speed up the training process, especially for large datasets.</td>\n",
        "    </tr>\n",
        "  </table>\n",
        "\n",
        "</body>\n",
        "</html>"
      ],
      "metadata": {
        "id": "15Wad1hXtI43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of the vocabulary is\", len(list(model_W2V.wv.key_to_index))) # Size of the vocabulary or number of unique words that the Word2Vec model has learned representations for."
      ],
      "metadata": {
        "id": "xUICMmW_tFSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Number of Unique Words or Vocabulary above (4692)"
      ],
      "metadata": {
        "id": "7SOAyzkftUyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"stock\"     # Selected word used frequently.\n",
        "model_W2V.wv[word] # Observe the word embedding of a selected word"
      ],
      "metadata": {
        "id": "kngxeos8tOgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"economy\"   # Second selected word\n",
        "model_W2V.wv[word] # Observe the word embedding of the second selected word"
      ],
      "metadata": {
        "id": "1UNLDG_ftYpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(model_W2V.wv.key_to_index.keys()) # Retrieve the words present in the --Word2Vec-- model's vocabulary\n",
        "wvs = model_W2V.wv[words].tolist()             # Retrieve word vectors for all the words present in the model's vocabulary\n",
        "word_vector_dict = dict(zip(words, wvs))       # Create a dictionary of words and their corresponding vectors"
      ],
      "metadata": {
        "id": "DgXxWtPNtdDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_vectorizer_Word2Vec(doc):\n",
        "    # Initializing a feature vector for the sentence\n",
        "    feature_vector = np.zeros((vec_size,), dtype=\"float64\")\n",
        "\n",
        "    # Create a list of words in the sentence that are present in the model vocabulary\n",
        "    words_in_vocab = [word for word in doc.split() if word in words]\n",
        "\n",
        "    # Add the vector representations of the words\n",
        "    for word in words_in_vocab:\n",
        "        feature_vector += np.array(word_vector_dict[word])\n",
        "\n",
        "    # Divide by the number of words to get the average vector\n",
        "    if len(words_in_vocab) != 0:\n",
        "        feature_vector /= len(words_in_vocab)\n",
        "\n",
        "    return feature_vector"
      ],
      "metadata": {
        "id": "vmQWC-5rtfqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe of the vectorized documents\n",
        "start = time.time()\n",
        "\n",
        "X_train_wv = pd.DataFrame(X_train[\"News\"].apply(average_vectorizer_Word2Vec).tolist(), columns=['Feature '+str(i) for i in range(vec_size)])\n",
        "X_val_wv = pd.DataFrame(X_val[\"News\"].apply(average_vectorizer_Word2Vec).tolist(), columns=['Feature '+str(i) for i in range(vec_size)])\n",
        "X_test_wv = pd.DataFrame(X_test[\"News\"].apply(average_vectorizer_Word2Vec).tolist(), columns=['Feature '+str(i) for i in range(vec_size)])\n",
        "\n",
        "end = time.time()\n",
        "print('Time taken ', (end-start))"
      ],
      "metadata": {
        "id": "cmFofXYbtjs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_wv.shape,'train split\\n', X_val_wv.shape,'validation split\\n', X_test_wv.shape,'test split\\n') # Train-Validatio-Test Splits"
      ],
      "metadata": {
        "id": "xwVVHP8Ptm7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 2 - GloVe"
      ],
      "metadata": {
        "id": "jRb0049Ttr81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the GloVe model (Stanford's) if it doesn't exist\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "# Convert GloVe to word2vec format\n",
        "glove_input_file = 'glove.6B.100d.txt'\n",
        "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)\n",
        "\n",
        "# Load the converted model\n",
        "filename = 'glove.6B.100d.txt.word2vec'\n",
        "glove_model = KeyedVectors.load_word2vec_format(filename, binary=False)"
      ],
      "metadata": {
        "id": "IPRec1gHttn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of the vocabulary is\", len(glove_model.index_to_key)) # Check the size of the vocabulary"
      ],
      "metadata": {
        "id": "W2zFIUiwtzv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"stock\"    # Select the word embedding for first word. A very frequently used word.\n",
        "glove_model[word] # View the word embedding of selected word"
      ],
      "metadata": {
        "id": "F2kEuyJCt3o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"economy\"  # Select the word embedding for a second word.\n",
        "glove_model[word] # View the word embedding of selected word"
      ],
      "metadata": {
        "id": "P4-uUP-bvM0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_words = glove_model.index_to_key                                                 # Retrieve the words present in the GloVe model's vocabulary\n",
        "glove_word_vector_dict = dict(zip(glove_model.index_to_key,list(glove_model.vectors))) # Create a dictionary of words and their corresponding vectors"
      ],
      "metadata": {
        "id": "MoBN0kfmvPdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each word can be represented by a 100-dimensional vector (100 features).\n",
        "vec_size=100 # Specifies the number of dimensions for the embedding space."
      ],
      "metadata": {
        "id": "botTHmqfvVbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_vectorizer_GloVe(doc):\n",
        "    # Initializing a feature vector for the sentence\n",
        "    feature_vector = np.zeros((vec_size,), dtype=\"float64\")\n",
        "\n",
        "    # Creating a list of words in the sentence that are present in the model vocabulary\n",
        "    words_in_vocab = [word for word in doc.split() if word in glove_words]\n",
        "\n",
        "    # adding the vector representations of the words\n",
        "    for word in words_in_vocab:\n",
        "        feature_vector += np.array(glove_word_vector_dict[word])\n",
        "\n",
        "    # Dividing by the number of words to get the average vector\n",
        "    if len(words_in_vocab) != 0:\n",
        "        feature_vector /= len(words_in_vocab)\n",
        "\n",
        "    return feature_vector"
      ],
      "metadata": {
        "id": "SBSAA1S8vdOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe of the vectorized documents\n",
        "start = time.time()\n",
        "\n",
        "X_train_gl = pd.DataFrame(X_train[\"News\"].apply(average_vectorizer_GloVe).tolist(), columns=['Feature '+str(i) for i in range(vec_size)]) # Apply GloVe on 'News' column for Training set.\n",
        "X_val_gl = pd.DataFrame(X_val[\"News\"].apply(average_vectorizer_GloVe).tolist(), columns=['Feature '+str(i) for i in range(vec_size)])     # Apply GloVe on 'News' column For Validation set.\n",
        "X_test_gl = pd.DataFrame(X_test[\"News\"].apply(average_vectorizer_GloVe).tolist(), columns=['Feature '+str(i) for i in range(vec_size)])   # Apply GloVe on 'News' column for Testing set.\n",
        "\n",
        "end = time.time()\n",
        "print('Time taken ', (end-start))"
      ],
      "metadata": {
        "id": "9SZSyPh2veYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Time taken to create a dataframe of the vectorized documents using GloVe \\033[1m{end - start:.6f} seconds.') # Rounded to 6 significant digits."
      ],
      "metadata": {
        "id": "r5end0brvhcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('For GloVe:\\n', X_train_gl.shape,'train split\\n', X_val_gl.shape,'validation split\\n', X_test_gl.shape,'test split\\n') # Train-Validatio-Test Splits"
      ],
      "metadata": {
        "id": "G_n9pBqqvwi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3 - Sentence Transformer"
      ],
      "metadata": {
        "id": "C_VyEpZyv2_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') # Defining the model for text classification, semantic search and sentiment analysis."
      ],
      "metadata": {
        "id": "B9Zjh0VCvy3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Setting the device to GPU if available, else CPU"
      ],
      "metadata": {
        "id": "ApwP9hPgxC26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the dataset splits with the Transformer Model\n",
        "start = time.time()\n",
        "\n",
        "X_train_st = model.encode(X_train[\"News\"].values, show_progress_bar=True, device=device) # Apply Sentence Transformer on 'News' column for the Training set.\n",
        "X_val_st = model.encode(X_val[\"News\"].values, show_progress_bar=True, device=device)     # Apply Sentence Transformer on 'News' column for the Validation set.\n",
        "X_test_st = model.encode(X_test[\"News\"].values, show_progress_bar=True, device=device)   # Apply Sentence Transformer on 'News' column for the Test set.\n",
        "\n",
        "end = time.time()\n",
        "print(f'Time taken using a Transformer \\033[1m{end - start:.6f} seconds.') # Rounded to 6 significant digits."
      ],
      "metadata": {
        "id": "A2PKRgkzJ2WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('For our Transformer model:\\n', X_train_st.shape,'train split\\n', X_val_st.shape,'validation split\\n', X_test_st.shape,'test split\\n') # Train-Validatio-Test Splits"
      ],
      "metadata": {
        "id": "T9_D5xG0J7MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation Criterion\n",
        "\n",
        "For each Model we will look for:\n",
        "\n",
        "- Accuracy and F1 Scores from Sentiment Predictions (Labels) by measuring the Confusion Matrix for each model and compare them.\n",
        "- Computational Cost: Consider the time and resources required to train and use each model. We can compare processing times for each model and compare them.\n",
        "- I will select the same Classifier when comparing all 3 models.\n",
        "- We could run more extensive training, (Colab permitting) by trying each of the following classifiers:\n",
        "  - GradientBoostingClassifier\n",
        "  - RandomForestClassifier\n",
        "  - DecisionTreeClassifier\n",
        "\n",
        "We Need to Consider these factors:\n",
        "- Vector Size: While not the sole determinant, larger vectors (like Transformer's 384) can potentially capture more complex relationships but may also be computationally more expensive.\n",
        "- Training Data: The quality and size of the data used to train each model significantly impacts performance."
      ],
      "metadata": {
        "id": "8DaPJPDwKDoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(model, predictors, target):\n",
        "    \"\"\"\n",
        "    Plot a confusion matrix to visualize the performance of a classification model.\n",
        "\n",
        "    Parameters:\n",
        "    actual (array-like): The true labels.\n",
        "    predicted (array-like): The predicted labels from the model.\n",
        "\n",
        "    Returns:\n",
        "    None: Displays the confusion matrix plot.\n",
        "    \"\"\"\n",
        "    pred = model.predict(predictors)  # Make predictions using the classifier.\n",
        "\n",
        "    cm = confusion_matrix(target, pred)  # Compute the confusion matrix.\n",
        "\n",
        "    plt.figure(figsize=(5, 4))  # Create a new figure with a specified size.\n",
        "    label_list = [0, 1,-1]  # Define the labels for the confusion matrix.\n",
        "    sns.heatmap(cm, annot=True, fmt='.0f', cmap='Blues', xticklabels=label_list, yticklabels=label_list)\n",
        "    # Plot the confusion matrix using a heatmap with annotations.\n",
        "\n",
        "    plt.ylabel('Actual')  # Label for the y-axis.\n",
        "    plt.xlabel('Predicted')  # Label for the x-axis.\n",
        "    plt.title('Confusion Matrix')  # Title of the plot.\n",
        "    plt.show()  # Display the plot."
      ],
      "metadata": {
        "id": "eOymfN3RJ_Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_performance_classification_sklearn(model, predictors, target):\n",
        "    \"\"\"\n",
        "    Compute various performance metrics for a classification model using sklearn.\n",
        "\n",
        "    Parameters:\n",
        "    model (sklearn classifier): The classification model to evaluate.\n",
        "    predictors (array-like): The independent variables used for predictions.\n",
        "    target (array-like): The true labels for the dependent variable.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: A DataFrame containing the computed metrics (Accuracy, Recall, Precision, F1-score).\n",
        "    \"\"\"\n",
        "    pred = model.predict(predictors)  # Make predictions using the classifier.\n",
        "\n",
        "    acc = accuracy_score(target, pred)  # Compute Accuracy.\n",
        "    recall = recall_score(target, pred,average='weighted')  # Compute Recall.\n",
        "    precision = precision_score(target, pred,average='weighted')  # Compute Precision.\n",
        "    f1 = f1_score(target, pred,average='weighted')  # Compute F1-score.\n",
        "\n",
        "    # Create a DataFrame to store the computed metrics.\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\n",
        "            \"Accuracy\": [acc],\n",
        "            \"Recall\": [recall],\n",
        "            \"Precision\": [precision],\n",
        "            \"F1\": [f1],\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return df_perf  # Return the DataFrame with the metrics."
      ],
      "metadata": {
        "id": "eXpCxVtYKtsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Untuned - Model Training"
      ],
      "metadata": {
        "id": "2Lbt-2_SK-L7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Untuned: Word2Vec"
      ],
      "metadata": {
        "id": "yGSaaZgiLC1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the model\n",
        "\n",
        "#Uncomment only one of the snippets related to fitting the model to the data\n",
        "\n",
        "#base_wv = GradientBoostingClassifier(random_state = 42)\n",
        "#base_wv = RandomForestClassifier(random_state=42)\n",
        "base_wv = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Fitting on train data\n",
        "base_wv.fit(X_train_wv, y_train)"
      ],
      "metadata": {
        "id": "bFXvjh_NKwyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(base_wv,X_train_wv,y_train) # Training"
      ],
      "metadata": {
        "id": "Ce21-RnmK2CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(base_wv,X_val_wv,y_val) # Validation"
      ],
      "metadata": {
        "id": "b9hOOmV1LIfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating different metrics on training data\n",
        "base_train_wv = model_performance_classification_sklearn(base_wv,X_train_wv,y_train)\n",
        "print(\"Training performance:\\n\", base_train_wv)"
      ],
      "metadata": {
        "id": "AgmzYgazLKuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating different metrics on validation data\n",
        "base_val_wv = model_performance_classification_sklearn(base_wv,X_val_wv,y_val)\n",
        "print(\"Validation performance:\\n\",base_val_wv)"
      ],
      "metadata": {
        "id": "vwCX_6sFLOGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Untuned: GloVe"
      ],
      "metadata": {
        "id": "iBvJuOPqMNpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the model\n",
        "\n",
        "#Uncomment only one of the snippets related to fitting the model to the data\n",
        "\n",
        "#base_wv = GradientBoostingClassifier(random_state = 42)\n",
        "#base_wv = RandomForestClassifier(random_state=42)\n",
        "base_gl = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Fitting on train data\n",
        "base_gl.fit(X_train_gl, y_train) #Complete the code to fit the chosen model on the train data"
      ],
      "metadata": {
        "id": "HJBsiT9FLSS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(base_gl,X_train_gl,y_train) # Confusion matrix for the train data on GloVe"
      ],
      "metadata": {
        "id": "FGp4blhFMUkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(base_gl,X_val_gl,y_val) # Confusion matrix for the validation data on GloVe"
      ],
      "metadata": {
        "id": "KNKp5aURMWxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating different metrics on training data\n",
        "base_train_gl=model_performance_classification_sklearn(base_gl,X_train_gl,y_train) # Calculate model performance for the training data on a GloVe Model\n",
        "print(\"Training performance:\\n\", base_train_gl)"
      ],
      "metadata": {
        "id": "6sZZk2EvMZBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating different metrics on validation data\n",
        "base_val_gl = model_performance_classification_sklearn(base_gl,X_val_gl,y_val) # Calculate model performance for the validation data on a GloVe model.\n",
        "print(\"Validation performance:\\n\",base_val_gl)"
      ],
      "metadata": {
        "id": "m2_sERIOMbro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Untuned: Sentence Transformer"
      ],
      "metadata": {
        "id": "HNKKLp8HMhmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the model\n",
        "\n",
        "#Uncomment only one of the snippets related to fitting the model to the data\n",
        "\n",
        "#base_wv = GradientBoostingClassifier(random_state = 42)\n",
        "#base_wv = RandomForestClassifier(random_state=42)\n",
        "base_st = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Fitting on train data\n",
        "base_st.fit(X_train_st, y_train) #Complete the code to fit the chosen model on the train data"
      ],
      "metadata": {
        "id": "gb6AZh1_MeHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(base_st,X_train_st,y_train) # Confusion matrix for the train data on our Transformer model."
      ],
      "metadata": {
        "id": "rzKocYcHMn5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(base_st,X_val_st,y_val) # Confusion matrix for the validation data on our Transformer model."
      ],
      "metadata": {
        "id": "Klcf--u3MqQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating different metrics on training data\n",
        "base_train_st=model_performance_classification_sklearn(base_st,X_train_st,y_train) # Model performance for the training data on our Transformer Model.\n",
        "print(\"Training performance:\\n\", base_train_st)"
      ],
      "metadata": {
        "id": "N3wGfEnHMsW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating different metrics on validation data\n",
        "base_val_st = model_performance_classification_sklearn(base_st,X_val_st,y_val)  # Model performance for the validation data on our Transformer Model.\n",
        "print(\"Validation performance:\\n\",base_val_st)"
      ],
      "metadata": {
        "id": "5fvQISwgNgt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<h2>Decision Tree Classifier Tuning Parameters</h2>\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Parameter</th>\n",
        "    <th>Description</th>\n",
        "    <th>Values</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>max_depth</td>\n",
        "    <td>Maximum depth of the tree</td>\n",
        "    <td>[3, 4, 5, 6]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>min_samples_split</td>\n",
        "    <td>Minimum number of samples required to split an internal node</td>\n",
        "    <td>[5, 7, 9, 11]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>max_features</td>\n",
        "    <td>Number of features considered when splitting a node</td>\n",
        "    <td>['log2', 'sqrt', 0.2, 0.4]</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "</body>\n",
        "</html>"
      ],
      "metadata": {
        "id": "Z5FQRjOpPfEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "# Choose the type of classifier.\n",
        "\n",
        "#Uncomment only one of the snippets corrrsponding to the base model trained previously\n",
        "\n",
        "#tuned_wv = GradientBoostingClassifier(random_state = 42)\n",
        "#tuned_wv = RandomForestClassifier(random_state=42)\n",
        "tuned_wv = DecisionTreeClassifier(random_state=42)\n",
        "tuned_gl = DecisionTreeClassifier(random_state=42)\n",
        "tuned_st = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "parameters = {\n",
        "    'max_depth': np.arange(3,7),\n",
        "    'min_samples_split': np.arange(5,12,2),\n",
        "    'max_features': ['log2', 'sqrt', 0.2, 0.4]\n",
        "}\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(tuned_wv, parameters, scoring='f1_weighted',cv=5,n_jobs=-1)\n",
        "grid_obj = grid_obj.fit(X_train_wv, y_train)\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time taken \",(end-start))\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "tuned_wv = grid_obj.best_estimator_"
      ],
      "metadata": {
        "id": "xveO3yPSNiuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tuned: Word2Vec"
      ],
      "metadata": {
        "id": "x1vdgaZLPTNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the best algorithm to the data.\n",
        "tuned_wv.fit(X_train_wv, y_train)"
      ],
      "metadata": {
        "id": "FXfjEBkhPcsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(tuned_wv,X_train_wv,y_train)"
      ],
      "metadata": {
        "id": "yHfP24BwPp53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(tuned_wv,X_val_wv,y_val)"
      ],
      "metadata": {
        "id": "jCeK_SEDPulE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating different metrics on training data\n",
        "tuned_train_wv=model_performance_classification_sklearn(tuned_wv,X_train_wv,y_train)\n",
        "print(\"Training performance:\\n\",tuned_train_wv)"
      ],
      "metadata": {
        "id": "uuL1RDMuPw2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating different metrics on validation data\n",
        "tuned_val_wv = model_performance_classification_sklearn(tuned_wv,X_val_wv,y_val)\n",
        "print(\"Validation performance:\\n\",tuned_val_wv)"
      ],
      "metadata": {
        "id": "4XBJNnf9PzNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tuned: GloVe"
      ],
      "metadata": {
        "id": "KFx3TXz1P3ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the best algorithm to the data.\n",
        "tuned_gl.fit(X_train_gl, y_train) # Fit the chosen model on the train data"
      ],
      "metadata": {
        "id": "7MlQQXIZP1Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(tuned_gl,X_train_gl, y_train) # Confusion matrix for the train data"
      ],
      "metadata": {
        "id": "TGo4VdXRP2nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(tuned_gl,X_val_gl,y_val) # Confusion matrix for the validation data"
      ],
      "metadata": {
        "id": "xIzE7Tp8QybW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics on training data\n",
        "tuned_train_gl=model_performance_classification_sklearn(tuned_gl,X_train_gl, y_train) # Model performance for the training data on GloVe model.\n",
        "print(\"Training performance:\\n\",tuned_train_gl)"
      ],
      "metadata": {
        "id": "uZxZQfEnQ0hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating different metrics on validation data\n",
        "tuned_val_gl = model_performance_classification_sklearn(tuned_gl,X_val_gl,y_val) # Model performance for the validation data on GloVe model.\n",
        "print(\"Validation performance:\\n\",tuned_val_gl)"
      ],
      "metadata": {
        "id": "Bj8QArfPQ21r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tuned: Sentence Transformer"
      ],
      "metadata": {
        "id": "j-JYPNYHRGYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the best algorithm to the data.\n",
        "tuned_st.fit(X_train_st, y_train) #Complete the code to fit the chosen model on the train data"
      ],
      "metadata": {
        "id": "0lOIqP08Q7Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(tuned_st,X_train_st,y_train) #Complete the code to plot the confusion matrix for the train data"
      ],
      "metadata": {
        "id": "6Ey8vDyERQtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(tuned_st,X_val_st,y_val) #Complete the code to plot the confusion matrix for the validation data"
      ],
      "metadata": {
        "id": "nur8lPCSRSux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics on training data\n",
        "tuned_train_st=model_performance_classification_sklearn(tuned_st,X_train_st,y_train) #C Model performance for the training data\n",
        "print(\"Training performance:\\n\",tuned_train_st)"
      ],
      "metadata": {
        "id": "HaVUesmlRVd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics on validation data\n",
        "tuned_val_st = model_performance_classification_sklearn(tuned_st,X_val_st,y_val) # Model performance for the validation data\n",
        "print(\"Validation performance:\\n\",tuned_val_st)"
      ],
      "metadata": {
        "id": "8bMVHnvjRYjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Selection\n",
        "\n",
        "#### Model Performance Summary"
      ],
      "metadata": {
        "id": "zPuggR2ZRfCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training performance comparison\n",
        "\n",
        "models_train_comp_df = pd.concat(\n",
        "    [base_train_wv.T,\n",
        "     base_train_gl.T,\n",
        "     base_train_st.T,\n",
        "     tuned_train_wv.T,\n",
        "     tuned_train_gl.T,\n",
        "     tuned_train_st.T,\n",
        "    ],axis=1\n",
        ")\n",
        "\n",
        "models_train_comp_df.columns = [\n",
        "    \"Base Model (Word2Vec)\",\n",
        "    \"Base Model (GloVe)\",\n",
        "    \"Base Model (Sentence Transformer)\",\n",
        "    \"Tuned Model (Word2Vec)\",\n",
        "    \"Tuned Model (GloVe)\",\n",
        "    \"Tuned Model (Sentence Transformer)\",\n",
        "]\n",
        "\n",
        "print(\"Training performance comparison:\")\n",
        "models_train_comp_df"
      ],
      "metadata": {
        "id": "FDmU-Fg4Ram0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation performance comparison\n",
        "\n",
        "models_val_comp_df = pd.concat(\n",
        "    [base_val_wv.T,\n",
        "     base_val_gl.T,\n",
        "     base_val_st.T,\n",
        "     tuned_val_wv.T,\n",
        "     tuned_val_gl.T,\n",
        "     tuned_val_st.T,\n",
        "     ],axis=1\n",
        ")\n",
        "\n",
        "models_val_comp_df.columns = [\n",
        "    \"Base Model (Word2Vec)\",\n",
        "    \"Base Model (GloVe)\",\n",
        "    \"Base Model (Sentence Transformer)\",\n",
        "    \"Tuned Model (Word2Vec)\",\n",
        "    \"Tuned Model (GloVe)\",\n",
        "    \"Tuned Model (Sentence Transformer)\",\n",
        "]\n",
        "\n",
        "print(\"Validation performance comparison:\")\n",
        "models_val_comp_df"
      ],
      "metadata": {
        "id": "xmAWE_elRm9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Performance Check on Testing dataset"
      ],
      "metadata": {
        "id": "X61AKBgKRxaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the best model to the test data.\n",
        "tuned_st.fit(X_test_wv, y_test) # Fit the chosen model on the test data."
      ],
      "metadata": {
        "id": "iLEjs7OwRrPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(tuned_st,X_test_wv,y_test) # Confusion matrix for the final model and test data."
      ],
      "metadata": {
        "id": "NN_WXOifR3WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating different metrics on test data\n",
        "final_model_test = model_performance_classification_sklearn(tuned_st,X_test_wv,y_test) # Final model's performance with the test data.\n",
        "print(\"Test performance for the final model:\\n\",final_model_test)"
      ],
      "metadata": {
        "id": "phycmMjQR5b0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Best Model: Sentence Transformer"
      ],
      "metadata": {
        "id": "k6_4UUQFTern"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Weekly News Summarization**"
      ],
      "metadata": {
        "id": "qOmgJS7jSBsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing and Importing the necessary libraries"
      ],
      "metadata": {
        "id": "ZvfnkC75UCXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It installs version 0.1.85 of the GPU llama-cpp-python library\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.85 --force-reinstall --no-cache-dir -q # Invoked as a shell command executed within Jupyter/Google Colab.\n",
        "\n",
        "# Installation for CPU llama-cpp-python\n",
        "# uncomment and run the following code in case GPU is not being used\n",
        "#!CMAKE_ARGS=\"-DLLAMA_CUBLAS=off\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.85 --force-reinstall --no-cache-dir -q"
      ],
      "metadata": {
        "id": "Bn_FMPW3R78B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from huggingface_hub import hf_hub_download          # Function to download the model from the Hugging Face model hub\n",
        "from llama_cpp import Llama                          # Importing the Llama class from the llama_cpp module\n",
        "import pandas as pd                                  # Importing the library for data manipulation\n",
        "from tqdm import tqdm                                # For progress bar related functionalities\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "I21O3yAEUFvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the data"
      ],
      "metadata": {
        "id": "jEvI1ruZUeti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data = pd.read_csv('https://raw.githubusercontent.com/ShauryaRawat10/Data-Science/main/Generative%20AI/Storage/stock_news.csv', engine='python')"
      ],
      "metadata": {
        "id": "3AjDV7iMUtl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = stock_news.copy()                            # Make a dtaframe copy for analysis"
      ],
      "metadata": {
        "id": "IvCtW8YBUVhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wordcount to check on tokens"
      ],
      "metadata": {
        "id": "i0B5rq0iU222"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcount of a text in a file named stock_news\n",
        "import re\n",
        "\n",
        "words = re.findall(r'\\b\\w+\\b', \", \".join(data['News'].astype(str)).lower()) #Find all words, convert to lower case\n",
        "print(\"Total word count:\", len(words))"
      ],
      "metadata": {
        "id": "3Gw1MKK9Um2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"  # Model path\n",
        "model_basename = \"mistral-7b-instruct-v0.2.Q6_K.gguf\"          # Model name\n",
        "\n",
        "model_path = hf_hub_download(                                  # Download the little model with 7.3 billion parameters\n",
        "    repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",          # Use the repo_id\n",
        "    filename=\"mistral-7b-instruct-v0.2.Q6_K.gguf\"              # Use this filename\n",
        ")                                                              # Examine progress (blue) until completion (green)"
      ],
      "metadata": {
        "id": "CvwAyVSTWmKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect runtime to GPU, a number of layers will be offloaded to the GPU for computations.\n",
        "llm = Llama(                                                   # Variable that will hold the instance of the Llama model. Llama is the instantiated class of the llama-cpp-python lybrary.\n",
        "    model_path=model_path,                                     # Path to the model, previously defined. This is typically a .bin file that contains the trained weights of the model.\n",
        "    n_gpu_layers=100,                                          # Number of layers transferred to GPU. Which ones will be listed as an output.\n",
        "    n_ctx=4500,                                                # Context window. It determines how much text (in tokens) the model can process or “remember” in a single pass.\n",
        ")"
      ],
      "metadata": {
        "id": "u7iaNZ4EWnR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregating data weekly\n",
        "data[\"Date\"] = pd.to_datetime(data['Date'])                                     # Convert the 'Date' column to datetime format."
      ],
      "metadata": {
        "id": "np84vFMoWsPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_grouped = data.groupby(pd.Grouper(key='Date', freq='W'))                 # Group the data by week using the 'Date' column."
      ],
      "metadata": {
        "id": "hk0wxdOjW3bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_grouped_full = weekly_grouped.apply(lambda x: x).reset_index(drop=True)  # Display all rows from the grouped Dataframe\n",
        "print(weekly_grouped_full)"
      ],
      "metadata": {
        "id": "i1-FjrBDW8AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate the \"News\" column with a '||' as a separator and their corresponding \"Volume\" and \"Label\" also separated by '||' and reset the index.\n",
        "# weekly_grouped = data.groupby(pd.Grouper(key='Date', freq='W')) # Group the data by week using the 'Date' column.\n",
        "weekly_aggregated = weekly_grouped.agg(\n",
        "    {\n",
        "        \"News\": lambda x: \" || \".join(x),\n",
        "        \"Volume\": lambda x: \" || \".join(map(str, x)),  # Assuming 'Value' needs string conversion\n",
        "        \"Label\": lambda x: \" || \".join(map(str, x)),  # Assuming 'Label' needs string conversion\n",
        "    }\n",
        ").reset_index()\n",
        "weekly_aggregated"
      ],
      "metadata": {
        "id": "QXCqKLtlW_GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare processed files ...\n",
        "print(\"\\nCompare processed files ...\")\n",
        "print(\"\\nweekly_grouped_full: \",weekly_grouped_full.shape, \"with Columns: \", weekly_grouped_full.columns)\n",
        "print(\"weekly_aggregated:    \",weekly_aggregated.shape,\"with Columns: \", weekly_aggregated.columns)"
      ],
      "metadata": {
        "id": "bvavLB6SXCCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_aggregated_copy = weekly_aggregated.copy()"
      ],
      "metadata": {
        "id": "fc8FMPF-ahDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Untilities"
      ],
      "metadata": {
        "id": "8ISFV7Q9Zg27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a function to parse the JSON output from the model\n",
        "def extract_json_data(json_str):\n",
        "    import json\n",
        "    try:\n",
        "        # Find the indices of the opening and closing curly braces\n",
        "        json_start = json_str.find('{')\n",
        "        json_end = json_str.rfind('}')\n",
        "\n",
        "        if json_start != -1 and json_end != -1:\n",
        "            extracted_category = json_str[json_start:json_end + 1]  # Extract the JSON object\n",
        "            data_dict = json.loads(extracted_category)\n",
        "            return data_dict\n",
        "        else:\n",
        "            print(f\"Warning: JSON object not found in response: {json_str}\")\n",
        "            return {}\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error parsing JSON: {e}\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "z06FU_fMXgE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Response function only creating Key Events\n",
        "def response_mistral_1(prompt, news):\n",
        "    model_output = llm(\n",
        "      f\"\"\"\n",
        "      [INST]\n",
        "      {prompt}\n",
        "      News Articles: {news}\n",
        "      [/INST]\n",
        "      \"\"\",\n",
        "      max_tokens=150,       # Set max tokens to limit response length. Limits the maximum number of tokens in the LLM's response to 150, controlling the length of the output.\n",
        "      temperature=0,        # Set temperature for minimum creativity. Sets the temperature to 0, results in more deterministic responses. Higher temperatures can lead to more diverse but imaginative outputs.\n",
        "      top_p=0.95,           # Set top_p for diversity. 0.95 means that the model will only select from the top 95% most probable tokens, leading to more focused and coherent responses.\n",
        "      top_k=50,             # Limit to top 50 tokens for better focus. Limits the number of considered tokens to the top 50 most probable ones, further refining the selection process.\n",
        "      stop=['INST'],        # Stop at the end of the instruction. Instructs the LLM to stop generating text when it encounters the [/INST] marker.\n",
        "      echo=False,\n",
        "    )\n",
        "\n",
        "    final_output = model_output[\"choices\"][0][\"text\"]\n",
        "\n",
        "    return final_output"
      ],
      "metadata": {
        "id": "tLCwVqmjaY1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Response function with 'Label' data incorporated\n",
        "def response_mistral_2(prompt, news, labels):\n",
        "    # Combine 'news' and 'labels' into a format you want to pass to the model\n",
        "    formatted_news_and_labels = \"\\n\".join([f\"News: {n} | Label: {l}\" for n, l in zip(news, labels)])\n",
        "\n",
        "    # Construct the prompt with both the news and the labels\n",
        "    model_output = llm(\n",
        "      f\"\"\"\n",
        "      [INST]\n",
        "      {prompt}\n",
        "      News Articles and Labels:\n",
        "      {formatted_news_and_labels}\n",
        "      [/INST]\n",
        "      \"\"\",\n",
        "      max_tokens=150,   # Set max tokens to limit response length\n",
        "      temperature=0,    # Set temperature for a more predictable response\n",
        "      top_p=0.95,       # Set top_p for diversity\n",
        "      top_k=50,         # Limit to top 50 tokens for better focus\n",
        "      stop=['INST'],    # Stop at the end of the instruction\n",
        "      echo=False,\n",
        "    )\n",
        "\n",
        "    final_output = model_output[\"choices\"][0][\"text\"]\n",
        "    return final_output"
      ],
      "metadata": {
        "id": "KVI3jELaZkc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news = weekly_aggregated_copy.loc[0, 'News'] # Using PROMPT 1 AND JSON"
      ],
      "metadata": {
        "id": "O1zDv1O3ZpxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(news.split(' '))) # Using PROMPT 1 AND JSON\n",
        "news"
      ],
      "metadata": {
        "id": "7ccvYi5AamyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------PROMPT 1 NOW WORKING ---\n",
        "prompt1 = \"\"\"\n",
        "You are an expert data analyst specializing in news analysis and sentiment analysis.\n",
        "\n",
        "Task: Analyze the provided news headlines and return the main topics within them.  Each event should be listed once, even if mentioned multiple times.\n",
        "\n",
        "Instructions:\n",
        "1. Read the news headline carefully to dentify the main subjects or entities mentioned in the news headline.\n",
        "3. Determine the key events or actions described in the headline.\n",
        "4. Extract relevant keywords that represent these same topics and summarize each.\n",
        "5. List these resulting summarized topics in a concise manner using an uniform numerical format like 1,2,3,4,5 always starting with numerical 1, per topic.\n",
        "6. Be sure to use uniform formatting for the output and always end each row with a period.\n",
        "\n",
        "Return the output results in JSON format with keys as the topic number and values as the actual topic\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7n7xUDuwascF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "summary = response_mistral_1(prompt1, news) # Using JSON, Using Simple Promts, Topics nicely generated.\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "xTqRvna8bPQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------PROMPT 2 -----\n",
        "prompt2 = \"\"\"\n",
        "You are an expert data analyst specializing in sentiment analysis.\n",
        "\n",
        "Task: Analyze the numbered Key Events and return corresponding Labels.\n",
        "\n",
        "Instructions:\n",
        "1. Read the numbered Key Events carefully.\n",
        "3. Determine the matching Labels key events and number them to match the Key Events.\n",
        "4. For each instance of a -1 create the text \"Negative News Event\" and for each instance of a 1 create the text \"Positive News Event\" matching each Key Events number.\n",
        "5. List these matching Labels now in text and make sure they still match the exact number of Key Events.\n",
        "6. Be sure to use uniform formatting for the output and always end each row with a period.\n",
        "\n",
        "\"\"\"\n",
        "# Return the output results in JSON format with keys as the topic number and values as the actual topic.\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "zYxCVI2TbAnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "summary = response_mistral_1(prompt2, data) # Using JSON, Using Simple Promts, Topics nicely generated.\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "Sb4LXbhdccSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "prompt3 = \"\"\"\n",
        "You are an expert data analyst specializing in stock market news article analysis that affects the financial market.\n",
        "Task: Analyze the news headlines and determine which news articles are positive or negative in sentiment.\n",
        "Instructions:\n",
        "1. Read the individual news article that is separated by ' || '.\n",
        "2. Identify if the article contains positive or negative sentiment based on optimistic or pessimistic indicators.\n",
        "2. Extract each article and create a summary based on the sentiment (Positive or Negative).\n",
        "2. Summarize results by grouping by date into weeks, include the individual news articles and count the number of Positive (1) and Negative (-1) sentiments.\n",
        "Output the results in JSON format.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "TYuZy8-xiJcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "summary = response_mistral_1(prompt, news)\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "ZRhdmAvLiPMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------PROMPT 2.1 ----\n",
        "prompt = \"\"\"\n",
        "You are an expert data analyst specializing in news analysis and sentiment analysis.\n",
        "\n",
        "Task: Analyze the provided news headlines and return the main topics for each of them.  Each event should be listed once, even if mentioned multiple times.\n",
        "\n",
        "Instructions:\n",
        "1. Read the news headline carefully to dentify the main subjects or entities mentioned in the news headline.\n",
        "3. Determine the key events or actions described in the headline.\n",
        "4. Extract relevant keywords that represent these same topics and summarize each.\n",
        "5. List these resulting summarized topics in a concise manner.\n",
        "6. Be sure to use uniform formatting for the output.\n",
        "\n",
        "\"\"\"\n",
        "# Return the output results in JSON format with keys as the topic number and values as the actual topic.\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "yR1T6JqRfW_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_nonjson = response_mistral_1(prompt, weekly_aggregated_copy) # This is where the rubber meets the road. Using the prompt properly <==========================  PROMPT #2\n",
        "print(summary_nonjson)"
      ],
      "metadata": {
        "id": "bjUaD2mNeOp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "data['Key Events'] = data['News'].progress_apply(lambda x: response_mistral_1(prompt,x))"
      ],
      "metadata": {
        "id": "haj_CbdKeuRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_1.head()"
      ],
      "metadata": {
        "id": "TrTGD1NRisu1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}